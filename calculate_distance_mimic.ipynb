{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.dataset2vec.data import get_preprocessing_pipeline\n",
    "\n",
    "from engine.dataset2vec.train import LightningWrapper as D2vWrapper\n",
    "from liltab.train.utils import LightningWrapper as LiltabWrapper\n",
    "from pathlib import Path\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/mimic/mini_holdout\")\n",
    "liltab_encoder_path = \"models/liltab.ckpt\"\n",
    "d2v_encoder_path = \"models/d2v.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5917/5917 [01:19<00:00, 74.25it/s]\n"
     ]
    }
   ],
   "source": [
    "tasks_paths = list(data_path.rglob(\"*test.csv\"))\n",
    "datasets = dict()\n",
    "\n",
    "for task_path in tqdm(tasks_paths):\n",
    "    task_name = str(task_path).split(\"/\")[-3]\n",
    "    if task_name not in datasets:\n",
    "        datasets[task_name] = []\n",
    "    df = pd.read_csv(task_path)\n",
    "    X, y = Tensor(get_preprocessing_pipeline().fit_transform(df.iloc[:, :-1]).values), Tensor(\n",
    "        df.iloc[:, -1].values\n",
    "    ).reshape(-1, 1)\n",
    "    datasets[task_name].append((X, y))\n",
    "\n",
    "datasets = list(datasets.values())\n",
    "n_datasets = len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(dataset):\n",
    "    idx = np.random.choice(len(dataset))\n",
    "    return dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/linux/sync/research/rethinking_encoder_warmstart/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "/mnt/linux/sync/research/rethinking_encoder_warmstart/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n"
     ]
    }
   ],
   "source": [
    "liltab_encoder = LiltabWrapper.load_from_checkpoint(liltab_encoder_path).model\n",
    "d2v_encoder = D2vWrapper.load_from_checkpoint(d2v_encoder_path).encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    \"liltab\": lambda X, y: liltab_encoder.encode_support_set(X, y).mean(dim=0),\n",
    "    \"d2v\": d2v_encoder,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_POINTS_PER_DATASET = 100\n",
    "NUMBER_OF_DATASETS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:02,  5.84it/s]\n"
     ]
    }
   ],
   "source": [
    "encoder = encoders[\"d2v\"]\n",
    "embeddings_all = []\n",
    "with torch.no_grad():\n",
    "    for i, dataset in tqdm(enumerate(datasets)):\n",
    "        for j in range(NUMBER_OF_POINTS_PER_DATASET):\n",
    "            X, y = get_sample(dataset)\n",
    "            if j == 0:\n",
    "                embeddings_all.append([])\n",
    "            embeddings_all[i].append(encoder(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1774)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids = []\n",
    "for i, embeddings in enumerate(embeddings_all):\n",
    "    centroids.append(torch.stack(embeddings).mean(dim=0))\n",
    "\n",
    "centroids_stacked = torch.stack(centroids)\n",
    "global_centroid = centroids_stacked.mean(dim=0)\n",
    "numerator = (\n",
    "    ((centroids_stacked - global_centroid) ** 2).sum()\n",
    "    * NUMBER_OF_POINTS_PER_DATASET\n",
    "    / NUMBER_OF_DATASETS\n",
    ")\n",
    "\n",
    "denominator = 0\n",
    "for centroid in centroids:\n",
    "    for embeddings in embeddings_all:\n",
    "        embeddings = torch.stack(embeddings)\n",
    "        denominator += ((embeddings - centroid) ** 2).sum()\n",
    "\n",
    "denominator /= (\n",
    "    NUMBER_OF_POINTS_PER_DATASET * NUMBER_OF_DATASETS - NUMBER_OF_DATASETS\n",
    ")\n",
    "numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:03,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "encoder = encoders[\"liltab\"]\n",
    "embeddings_all = []\n",
    "with torch.no_grad():\n",
    "    for i, dataset in tqdm(enumerate(datasets)):\n",
    "        for j in range(100):\n",
    "            X, y = get_sample(dataset)\n",
    "            if j == 0:\n",
    "                embeddings_all.append([])\n",
    "            embeddings_all[i].append(encoder(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2849)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids = []\n",
    "for i, embeddings in enumerate(embeddings_all):\n",
    "    centroids.append(torch.stack(embeddings).mean(dim=0))\n",
    "\n",
    "centroids_stacked = torch.stack(centroids)\n",
    "global_centroid = centroids_stacked.mean(dim=0)\n",
    "numerator = (\n",
    "    ((centroids_stacked - global_centroid) ** 2).sum()\n",
    "    * NUMBER_OF_POINTS_PER_DATASET\n",
    "    / NUMBER_OF_DATASETS\n",
    ")\n",
    "\n",
    "denominator = 0\n",
    "for centroid in centroids:\n",
    "    for embeddings in embeddings_all:\n",
    "        embeddings = torch.stack(embeddings)\n",
    "        denominator += ((embeddings - centroid) ** 2).sum()\n",
    "\n",
    "denominator /= (\n",
    "    NUMBER_OF_POINTS_PER_DATASET * NUMBER_OF_DATASETS - NUMBER_OF_DATASETS\n",
    ")\n",
    "numerator / denominator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
